{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KGCNcolab.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmuysiF+8106iaQuVqjRFS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ML6X8u4tAjPN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598402241676,"user_tz":-120,"elapsed":887,"user":{"displayName":"kubzz piepie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguG3NRvvZVq4Ff-etlYr6PrbQqZKATQm-BZgm4yw=s64","userId":"12323577025605747657"}},"outputId":"d7388ff0-cfa7-4d3b-c6c8-f3f4ef2f5031"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2j_5uoJEAlSt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598400846823,"user_tz":-120,"elapsed":1563,"user":{"displayName":"kubzz piepie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguG3NRvvZVq4Ff-etlYr6PrbQqZKATQm-BZgm4yw=s64","userId":"12323577025605747657"}},"outputId":"080175f7-dab7-4fc1-b565-9114a9aa5498"},"source":["\n","!git init\n","!git config --global user.email \"kuba.pietrak.94@gmail.com\"\n","!git config --global user.name \"Qbbz\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reinitialized existing Git repository in /content/.git/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ry3CDwP4CC2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1598400847082,"user_tz":-120,"elapsed":1804,"user":{"displayName":"kubzz piepie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguG3NRvvZVq4Ff-etlYr6PrbQqZKATQm-BZgm4yw=s64","userId":"12323577025605747657"}},"outputId":"caaacd7f-b947-4219-a4d2-1945ab82045f"},"source":["!git clone -b colab-branch https://github.com/Qbbz/SSP.git\n","!git pull https://github.com/Qbbz/SSP.git colab-branch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'SSP' already exists and is not an empty directory.\n","From https://github.com/Qbbz/SSP\n"," * branch            colab-branch -> FETCH_HEAD\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jWIdWxv9CLet","colab_type":"code","colab":{}},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYg-oUQOH1RO","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598402386768,"user_tz":-120,"elapsed":696,"user":{"displayName":"kubzz piepie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GguG3NRvvZVq4Ff-etlYr6PrbQqZKATQm-BZgm4yw=s64","userId":"12323577025605747657"}},"outputId":"a24c38b8-ed63-41d4-fa8b-b157f5f8e2f6"},"source":["import copy\n","import inspect\n","import time\n","import pickle\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import networkx as nx\n","import pandas as pd\n","import os\n","\n","from grakn.client import GraknClient\n","from KGCN.pipeline_mod import pipeline\n","#from kglib.kgcn.pipeline.pipeline import pipeline\n","from kglib.utils.graph.iterate import multidigraph_data_iterator\n","from kglib.utils.graph.query.query_graph import QueryGraph\n","from kglib.utils.grakn.type.type import get_thing_types, get_role_types\n","#from kglib.utils.graph.thing.queries_to_graph import build_graph_from_queries\n","from kglib.utils.graph.thing.queries_to_graph import combine_2_graphs, combine_n_graphs, concept_dict_from_concept_map\n","from kglib.utils.grakn.object.thing import build_thing\n","from kglib.utils.graph.thing.concept_dict_to_graph import concept_dict_to_graph\n","from sklearn.model_selection import train_test_split\n","\n","### TENSORFLOW CONFIGURATION\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","# Choose Config\n","use_tpu = False #@param {type:\"boolean\"} # TPU is not supported on TF 1.14\n","use_gpu = True \n","\n","# TPU Config\n","if use_tpu:\n","  assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n","  if 'COLAB_TPU_ADDR' in os.environ:\n","    TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR']) #tpu address\n","  else:\n","    TF_MASTER=''\n","\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","\n","# GPU Config\n","if use_gpu:\n","  config = tf.ConfigProto()\n","  config.gpu_options.allow_growth=True\n","  sess = tf.Session(config=config)\n","  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU'))) # Test tf for GPU acceleration\n","\n","import warnings\n","import matplotlib.cbook\n","warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation) #filter out mpl warnings\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN) #filter out annoying messages about name format with ':'\n","import sys\n","from pathlib import Path\n","from mylib.data_prep import LoadData, FeatDuct, UndersampleData\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Tensorflow version 1.14.0\n","Num GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_qMgyJ1wY0YX","colab_type":"code","colab":{}},"source":["PATH = os.getcwd()\n","DATAPATH = Path(PATH+\"/data/\")\n","ALLDATA = LoadData(DATAPATH)\n","ALLDATA = FeatDuct(ALLDATA, Input_Only = True) #leave only model input\n","PROCESSED_DATA = pd.read_csv(str(DATAPATH)+\"/ducts_data.csv\")\n","\n","KEYSPACE =  \"kgcn_schema_full\" #\"kgcn500n2500\" #\"ssp_schema_slope0\"  #\"sampled_ssp_schema_kgcn\"\n","URI = \"localhost:48555\"\n","SAVEPATH = str(DATAPATH) + \"/nx_500n2500_bias/\" #nx_500n2500\n","\n","# Existing elements in the graph are those that pre-exist in the graph, and should be predicted to continue to exist\n","PREEXISTS = 0\n","# Candidates are neither present in the input nor in the solution, they are negative samples\n","CANDIDATE = 1\n","# Elements to infer are the graph elements whose existence we want to predict to be true, they are positive samples\n","TO_INFER = 2\n","\n","# Categorical Attribute types and the values of their categories\n","ses = ['Winter', 'Spring', 'Summer', 'Autumn']\n","locations = []\n","for ssp in ALLDATA['profile']:\n","    season = next((s for s in ses if s in ssp), False)\n","    location = ssp.replace(season, '')[:-1]\n","    location = location.replace(' ', '-')\n","    locations.append(location)\n","loc = np.unique(locations).tolist()\n","\n","# Categorical Attributes and lists of their values\n","CATEGORICAL_ATTRIBUTES = {'season': ses,\n","                          'location': loc}\n","                          #duct_type': [\"NotDuct\",\"SLD\",\"DC\"]}\n","\n","# Continuous Attribute types and their min and max values\n","CONTINUOUS_ATTRIBUTES = {'depth': (0, 1500), \n","                         'num_rays': (500, 2500), \n","                         'slope': (-2, 2), \n","                         'bottom_type': (1,2),\n","                         'length': (0, 44000),\n","                         'SSP_value':(1463.486641,1539.630391),\n","                         'grad': (-0.290954924,0.040374179),\n","                         'number_of_ducts': (1,2)}\n","\n","TYPES_TO_IGNORE = ['candidate-convergence', 'scenario_id', 'probability_exists', 'probability_nonexists', 'probability_preexists']\n","ROLES_TO_IGNORE = ['candidate_resolution', 'candidate_scenario']\n","\n","# The learner should see candidate relations the same as the ground truth relations, so adjust these candidates to\n","# look like their ground truth counterparts\n","\n","TYPES_AND_ROLES_TO_OBFUSCATE = {'candidate-convergence': 'convergence',\n","                                'candidate_resolution': 'minimum_resolution',\n","                                'candidate_scenario': 'converged_scenario'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPW7qJdSZiR6","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","def build_graph_from_queries(query_sampler_variable_graph_tuples, grakn_transaction,\n","                             concept_dict_converter=concept_dict_to_graph, infer=True):\n","    \"\"\"\n","    Builds a graph of Things, interconnected by roles (and *has*), from a set of queries and graphs representing those\n","    queries (variable graphs)of those queries, over a Grakn transaction\n","\n","    Args:\n","        infer: whether to use Grakn's inference engine\n","        query_sampler_variable_graph_tuples: A list of tuples, each tuple containing a query, a sampling function,\n","            and a variable_graph\n","        grakn_transaction: A Grakn transaction\n","        concept_dict_converter: The function to use to convert from concept_dicts to a Grakn model. This could be\n","            a typical model or a mathematical model\n","\n","    Returns:\n","        A networkx graph\n","    \"\"\"\n","    query_concept_graphs = []\n","\n","    for query, sampler, variable_graph in query_sampler_variable_graph_tuples:\n","\n","        concept_maps = sampler(grakn_transaction.query(query, infer=infer))\n","        concept_dicts = [concept_dict_from_concept_map(concept_map) for concept_map in concept_maps]\n","        #TODO: Implement removal of NotDuct cases at NetworkX level instead of query workaround\n","        \"\"\"\n","        #print(concept_dicts)\n","        notaduct = 0\n","        for cd in concept_dicts:\n","            print(cd)\n","            for variable, thing in cd.items(): #key, value\n","                if variable == 'gd' and thing.value == 0.0:#and '0.0' in value:\n","                    print(variable, thing.value)\n","                    #cd.pop('gd')\n","                    #cd.pop('dct')\n","                    #cd.pop('SSP-channel')\n","                    #cd.pop()\n","                    #val = 'grad' in cd['gd']\n","                    #print(val)\n","                    #if '0.0' in value:\n","                    #    print('asdasda')\n","        \"\"\"\n","        answer_concept_graphs = []\n","        for concept_dict in concept_dicts:\n","            try:\n","                answer_concept_graphs.append(concept_dict_converter(concept_dict, variable_graph))\n","            except ValueError as e:\n","                raise ValueError(str(e) + f'Encountered processing query:\\n \\\"{query}\\\"')\n","\n","        if len(answer_concept_graphs) > 1:\n","            query_concept_graph = combine_n_graphs(answer_concept_graphs)\n","            query_concept_graphs.append(query_concept_graph)\n","        else:\n","            if len(answer_concept_graphs) > 0:\n","                query_concept_graphs.append(answer_concept_graphs[0])\n","            else:\n","                warnings.warn(f'There were no results for query: \\n\\\"{query}\\\"\\nand so nothing will be added to the '\n","                              f'graph for this query')\n","\n","    if len(query_concept_graphs) == 0:\n","        # Raise exception when none of the queries returned any results\n","        raise RuntimeError(f'The graph from queries: {[query_sampler_variable_graph_tuple[0] for query_sampler_variable_graph_tuple in query_sampler_variable_graph_tuples]}\\n'\n","                           f'could not be created, since none of these queries returned results')\n","\n","    concept_graph = combine_n_graphs(query_concept_graphs)\n","    #TODO: Remove NotDuct result from NetworkX graph completely: entity duct, attr grad 0, depth 0\n","     \n","    \n","    return concept_graph\n","\n","def create_concept_graphs(example_indices, grakn_session, savepath):\n","    \"\"\"\n","    Builds an in-memory graph for each example, with an scenario_id as an anchor for each example subgraph.\n","    Args:\n","        example_indices: The values used to anchor the subgraph queries within the entire knowledge graph\n","        =>> SCENARIO_ID\n","        grakn_session: Grakn Session\n","\n","    Returns:\n","        In-memory graphs of Grakn subgraphs\n","        \n","    Outline:    \n","    For scnenario_id with open grakn session:\n","        0. check if the nx.graph for example doesn't exists already in the output directory\n","        if yes: load nx.graph from pickle file\n","        if no: \n","            1. get_query_handles()\n","            2. build_graph_from_queries()\n","            3. obfuscate_labels() whatever it means\n","            4. graph.name = scenario_idx\n","            5. save ns.graph as pickle file\n","            6. append graph to list of graphs and return the list as func. output\n","            \n","\n","    \"\"\"\n","    \n","    graphs = []\n","    infer = True\n","    total = len(example_indices)\n","    # finds scenarios idx without ducts\n","    not_duct_idx = []\n","    for idx, sld, dc in zip(range(len(PROCESSED_DATA)),PROCESSED_DATA['SLD_depth'],PROCESSED_DATA['DC_axis']):\n","        if np.isnan(sld) and np.isnan(dc):\n","            not_duct_idx.append(idx)\n","        \n","    for it, scenario_idx in enumerate(example_indices):\n","        graph_filename = f'graph_{scenario_idx}.gpickle'\n","        if not os.path.exists(str(savepath)+\"/\"+graph_filename):\n","            print(f'[{it+1}|{total}] Creating graph for example {scenario_idx}')\n","            graph_query_handles = get_query_handles(scenario_idx, not_duct_idx)\n","            #print(graph_query_handles)\n","            with grakn_session.transaction().read() as tx:\n","                # Build a graph from the queries, samplers, and query graphs\n","                graph = build_graph_from_queries(graph_query_handles, tx, infer=infer)\n","    \n","            obfuscate_labels(graph, TYPES_AND_ROLES_TO_OBFUSCATE)\n","    \n","            graph.name = scenario_idx\n","            nx.write_gpickle(graph, savepath+graph_filename)\n","        \n","        else:\n","            print(f'[{it+1}|{total}] NetworkX graph loaded {graph_filename}')\n","            graph = nx.read_gpickle(savepath+graph_filename)    \n","        \n","        graphs.append(graph)\n","\n","        #TODO: SWITCH plot NetworkX graphs \n","        #new_graph = nx.Graph(graph)\n","        #nx.draw(new_graph, with_labels=True)\n","        #plt.show()\n","    return graphs\n","\n","def obfuscate_labels(graph, types_and_roles_to_obfuscate):\n","    # Remove label leakage - change type labels that indicate candidates into non-candidates\n","    for data in multidigraph_data_iterator(graph):\n","        for label_to_obfuscate, with_label in types_and_roles_to_obfuscate.items():\n","            if data['type'] == label_to_obfuscate:\n","                data.update(type=with_label)\n","                break\n","\n","def get_query_handles(scenario_idx, not_duct_idx):\n","        \n","    \"\"\"\n","    Creates an iterable, each element containing a Graql query, a function to sample the answers, and a QueryGraph\n","    object which must be the Grakn graph representation of the query. This tuple is termed a \"query_handle\"\n","    Args:\n","        scenario_idx: A uniquely identifiable attribute value used to anchor the results of the queries to a specific subgraph\n","    Returns:\n","        query handles\n","    \"\"\"\n","    # === Query variables ===\n","    conv, scn, ray, nray, src, dsrc, seg, dseg, l, s, srcp, bathy, bt, ssp, loc, ses,\\\n","    sspval, dsspmax, speed, dssp, dct, ddct, gd, duct, nod = 'conv','scn','ray', 'nray',\\\n","    'src', 'dsrc', 'seg', 'dseg','l','s','srcp','bathy','bt','ssp','loc','ses',\\\n","    'sspval','dsspmax','speed','dssp','dct','ddct','gd','duct','nod'\n","    # dt, 'dt'\n","    \n","    \n","    # === Candidate Convergence ===\n","    candidate_convergence_query = inspect.cleandoc(f'''match\n","           $scn isa sound-propagation-scenario, has scenario_id {scenario_idx};'''\n","           '''$ray isa ray-input, has num_rays $nray;\n","           $conv(candidate_scenario: $scn, candidate_resolution: $ray) isa candidate-convergence; \n","           get;''')    \n"," \n","    candidate_convergence_query_graph = (QueryGraph()\n","                                       .add_vars([conv], CANDIDATE)\n","                                       .add_vars([scn, ray, nray], PREEXISTS)\n","                                       .add_has_edge(ray, nray, PREEXISTS)\n","                                       .add_role_edge(conv, scn, 'candidate_scenario', CANDIDATE)\n","                                       .add_role_edge(conv, ray, 'candidate_resolution', CANDIDATE))\n","\n","    \n","    if scenario_idx not in not_duct_idx:\n","        # === Convergence: SCN with ducts ===    \n","        convergence_query_full = inspect.cleandoc(\n","            f'''match \n","            $scn isa sound-propagation-scenario, has scenario_id {scenario_idx};'''\n","            '''$ray isa ray-input, has num_rays $nray; \n","            $src isa source, has depth $dsrc; \n","            $seg isa bottom-segment, has depth $dseg, has length $l, has slope $s;\n","            $conv(converged_scenario: $scn, minimum_resolution: $ray) isa convergence;\n","            $srcp(defined_by_src: $scn, define_src: $src) isa src-position;\n","            $bathy(defined_by_bathy: $scn, define_bathy: $seg) isa bathymetry, has bottom_type $bt;\n","            $ssp isa SSP-vec, has location $loc, has season $ses, has SSP_value $sspval, has depth $dsspmax;\n","            $dct isa duct, has depth $ddct, has grad $gd;\n","            $speed(defined_by_SSP: $scn, define_SSP: $ssp) isa sound-speed;\n","            $duct(find_channel: $ssp, channel_exists: $dct) isa SSP-channel, has number_of_ducts $nod;\n","            $sspval has depth $dssp;\n","            {$dssp == $dsrc;} or {$dssp == $dseg;} or {$dssp == $ddct;} or {$dssp == $dsspmax;}; \n","            get;'''\n","            )\n","        # has duct_type $dt,\n","        \n","        convergence_query_full_graph = (QueryGraph()\n","                                 .add_vars([conv], TO_INFER)\n","                                 .add_vars([scn, ray, nray, src, dsrc, seg, dseg, \\\n","                                            l, s, srcp, bathy, bt, ssp, loc, ses, \\\n","                                            sspval, dsspmax, speed, dssp, dct, ddct,\\\n","                                            gd, duct, nod], PREEXISTS) #dt\n","                                 .add_has_edge(ray, nray, PREEXISTS)\n","                                 .add_has_edge(src, dsrc, PREEXISTS)\n","                                 .add_has_edge(seg, dseg, PREEXISTS)\n","                                 .add_has_edge(seg, l, PREEXISTS)\n","                                 .add_has_edge(seg, s, PREEXISTS)\n","                                 .add_has_edge(ssp, loc, PREEXISTS)\n","                                 .add_has_edge(ssp, ses, PREEXISTS)\n","                                 .add_has_edge(ssp, sspval, PREEXISTS)\n","                                 .add_has_edge(ssp, dsspmax, PREEXISTS)\n","                                 .add_has_edge(dct, ddct, PREEXISTS)\n","                                 #.add_has_edge(dct, dt, PREEXISTS)\n","                                 .add_has_edge(dct, gd, PREEXISTS)\n","                                 .add_has_edge(bathy, bt, PREEXISTS)\n","                                 .add_has_edge(duct, nod, PREEXISTS)\n","                                 .add_has_edge(sspval, dssp, PREEXISTS)\n","                                 .add_role_edge(conv, scn, 'converged_scenario', TO_INFER)\n","                                 .add_role_edge(conv, ray, 'minimum_resolution', TO_INFER)\n","                                 .add_role_edge(srcp, scn, 'defined_by_src', PREEXISTS)\n","                                 .add_role_edge(srcp, src, 'define_src', PREEXISTS)\n","                                 .add_role_edge(bathy, scn, 'defined_by_bathy', PREEXISTS)\n","                                 .add_role_edge(bathy, seg, 'define_bathy', PREEXISTS)\n","                                 .add_role_edge(speed, scn, 'defined_by_SSP', PREEXISTS)\n","                                 .add_role_edge(speed, ssp, 'define_SSP', PREEXISTS)\n","                                 .add_role_edge(duct, ssp, 'find_channel', PREEXISTS)\n","                                 .add_role_edge(duct, dct, 'channel_exists', PREEXISTS)\n","                                 )\n","        return [\n","            (convergence_query_full, lambda x: x, convergence_query_full_graph),\n","            (candidate_convergence_query, lambda x: x, candidate_convergence_query_graph)\n","            ]\n","    \n","    \n","    else:        \n","        # === Convergence: SCN with\\without ducts ===\n","        convergence_query_reduced = inspect.cleandoc(\n","                f'''match \n","                $scn isa sound-propagation-scenario, has scenario_id {scenario_idx};'''\n","                '''$ray isa ray-input, has num_rays $nray; \n","                $src isa source, has depth $dsrc; \n","                $seg isa bottom-segment, has depth $dseg, has length $l, has slope $s;\n","                $conv(converged_scenario: $scn, minimum_resolution: $ray) isa convergence;\n","                $srcp(defined_by_src: $scn, define_src: $src) isa src-position;\n","                $bathy(defined_by_bathy: $scn, define_bathy: $seg) isa bathymetry, has bottom_type $bt;\n","                $ssp isa SSP-vec, has location $loc, has season $ses, has SSP_value $sspval, has depth $dsspmax;\n","                $speed(defined_by_SSP: $scn, define_SSP: $ssp) isa sound-speed;\n","                $sspval has depth $dssp;\n","                {$dssp == $dsrc;} or {$dssp == $dseg;} or {$dssp == $dsspmax;}; \n","                get;'''\n","                )\n","            \n","        convergence_query_reduced_graph = (QueryGraph()\n","                                 .add_vars([conv], TO_INFER)\n","                                 .add_vars([scn, ray, nray, src, dsrc, seg, dseg, \\\n","                                            l, s, srcp, bathy, bt, ssp, loc, ses, \\\n","                                            sspval, dsspmax, speed, dssp], PREEXISTS)\n","                                 .add_has_edge(ray, nray, PREEXISTS)\n","                                 .add_has_edge(src, dsrc, PREEXISTS)\n","                                 .add_has_edge(seg, dseg, PREEXISTS)\n","                                 .add_has_edge(seg, l, PREEXISTS)\n","                                 .add_has_edge(seg, s, PREEXISTS)\n","                                 .add_has_edge(ssp, loc, PREEXISTS)\n","                                 .add_has_edge(ssp, ses, PREEXISTS)\n","                                 .add_has_edge(ssp, sspval, PREEXISTS)\n","                                 .add_has_edge(ssp, dsspmax, PREEXISTS)\n","                                 .add_has_edge(bathy, bt, PREEXISTS)                           \n","                                 .add_has_edge(sspval, dssp, PREEXISTS)\n","                                 .add_role_edge(conv, scn, 'converged_scenario', TO_INFER) #TO_INFER VS CANDIDATE BELOW\n","                                 .add_role_edge(conv, ray, 'minimum_resolution', TO_INFER)\n","                                 .add_role_edge(srcp, scn, 'defined_by_src', PREEXISTS)\n","                                 .add_role_edge(srcp, src, 'define_src', PREEXISTS)\n","                                 .add_role_edge(bathy, scn, 'defined_by_bathy', PREEXISTS)\n","                                 .add_role_edge(bathy, seg, 'define_bathy', PREEXISTS)\n","                                 .add_role_edge(speed, scn, 'defined_by_SSP', PREEXISTS)\n","                                 .add_role_edge(speed, ssp, 'define_SSP', PREEXISTS)\n","                                 )    \n","            \n","\n","        return [\n","            (convergence_query_reduced, lambda x: x, convergence_query_reduced_graph),\n","            (candidate_convergence_query, lambda x: x, candidate_convergence_query_graph)\n","            ]\n","\n","def write_predictions_to_grakn(graphs, tx, commit = True):\n","    \"\"\"\n","    Take predictions from the ML model, and insert representations of those predictions back into the graph.\n","\n","    Args:\n","        graphs: graphs containing the concepts, with their class predictions and class probabilities\n","        tx: Grakn write transaction to use\n","\n","    Returns: None\n","\n","    \"\"\"\n","    \n","    #TODO: Revise these loops and see why nothing is being predicted as data['prediction']=2 (exists?)\n","    for graph in graphs:\n","        for node, data in graph.nodes(data=True):\n","            if data['prediction'] == 2:\n","                concept = data['concept']\n","                concept_type = concept.type_label\n","                if concept_type == 'convergence' or concept_type == 'candidate-convergence':\n","                    neighbours = graph.neighbors(node)\n","\n","                    for neighbour in neighbours:\n","                        concept = graph.nodes[neighbour]['concept']\n","                        if concept.type_label == 'sound-propagation-scenario':\n","                            scenario = concept\n","                        else:\n","                            ray = concept\n","\n","                    p = data['probabilities']\n","                    query = (f'match '\n","                             f'$scn id {scenario.id}; '\n","                             f'$ray id {ray.id}; '\n","                             f'insert '\n","                             f'$conv(converged_scenario: $scn, minimum_resolution: $ray) isa convergence, '\n","                             f'has probability_exists {p[2]:.3f}, '\n","                             f'has probability_nonexists {p[1]:.3f}, '  \n","                             f'has probability_preexists {p[0]:.3f};')\n","                    print(query)\n","                    tx.query(query)\n","    if commit:\n","        tx.commit()\n","\n","import re\n","def ubuntu_rand_fix(savepath):\n","    #savepath = PATH + '/networkx/'\n","    graphfiles = [f for f in os.listdir(savepath) if os.path.isfile(os.path.join(savepath, f))]\n","    example_idx = []\n","    for gfile in graphfiles:\n","        idx = re.findall(r'\\d+', gfile)[0]    \n","        example_idx.append(idx)\n","    return example_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"79OBzVkGZjcf","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","def prepare_data(session, data, train_split, validation_split, savepath, ubuntu_fix = True):\n","    \"\"\"\n","    Args:\n","        data: full dataset with sorted scenario_id's that will be used for querying grakn\n","        train_split: size of the training set; \n","        validaton_split: size of the validaton set subtracted from the test set; \n","    \n","        Test set is further split down into test and validation so that\n","        test_set size = (1-train_split)*(1-validation_split)\n","        so i.e. train_split = 0.7, validation_split=0.33 results in:\n","        70% training set, 20.1% test set, 9.9% validation set\n","    \"\"\"\n","    seed = 123\n","    \n","    y = data.pop('num_rays').to_frame()\n","    X = data\n","    # divide whole dataset into stratified train\\test \n","    X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, stratify=y, shuffle = True, random_state = seed, test_size=1-train_split)\n","    #if validation_split > 0:\n","    #divide test dataset into stratified test\\validation subsets\n","    #X_test, X_val, y_test, y_val = train_test_split(\n","    #X_test, y_test, stratify=y_test, shuffle = True, random_state = seed, test_size=validation_split)\n","    \n","    training_data = [X_train, y_train]\n","    testing_data = [X_test, y_test]\n","    #validation_data = [X_val, y_val]\n","    \n","    # data was split and shuffled while mainating original indices \n","    # now the training and test set indices are merged once again\n","    # and will be split again inside the grakn pipeline until tr_ge_split, without shuffle\n","    \n","    num_tr_graphs = len(X_test) + len(X_train)   \n","    #num_val_graphs = len(X_val)\n","    example_idx_tr = X_train.index.tolist() + X_test.index.tolist() #training and test sets indices merged for training\n","\n","    # rand in linux and windows generates different number in effect the data selected in windows is different than ubuntu\n","    if ubuntu_fix:\n","        example_idx_tr = ubuntu_rand_fix(savepath)\n","    #example_idx_val = X_val.index.tolist()\n","    tr_ge_split = int(num_tr_graphs * train_split)  # Define graph number split in train graphs[:tr_ge_split] and test graphs[tr_ge_split:] sets\n","    #val_ge_split = int(len(X_val)*(1-validation_split))\n","    print(f'\\nCREATING {num_tr_graphs} TRAINING\\TEST GRAPHS')\n","    train_graphs = create_concept_graphs(example_idx_tr, session, savepath)  # Create validation graphs in networkX\n","    #print(f'\\nCREATING {num_val_graphs} VALIDATION GRAPHS')\n","    #val_graphs = create_concept_graphs(example_idx_val, session, savepath) # Create training graphs in networkX\n","    \n","    return  train_graphs, tr_ge_split, training_data, testing_data #, val_graphs,  val_ge_split\n","\n","def go_train(train_graphs, tr_ge_split, **kwargs):\n","    \"\"\"\n","    Args:\n","           \n","    Parameters\n","    ----------\n","    train_graphs : networkx graphs obtained from grakn queries - the set contains both train and test graphs!\n","    tr_ge_split : int. value marking the number of training graphs in train_graphs\n","    save_fle : model filename to be saved as tf. checkpoin\n","    **kwargs : TYPE\n","\n","    Returns:\n","    ge_graphs: Encoded in-memory graphs of Grakn concepts for generalisation\n","    solveds_tr: training fraction examples solved correctly\n","    solveds_ge: test/generalization fraction examples solved correctly\n","\n","    \"\"\"\n","    # Run the pipeline with prepared networkx graph\n","    #ge_graphs, solveds_tr, solveds_ge, graphs_enc, input_graphs, target_graphs, feed_dict \n","    ge_graphs, solveds_tr, solveds_ge = pipeline(graphs = train_graphs,             \n","                                                tr_ge_split = tr_ge_split,                         \n","                                                do_test = False,\n","                                                **kwargs)\n","    \n","    training_evals= [solveds_tr, solveds_ge]   \n","    return ge_graphs, solveds_tr, solveds_ge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vGTMr8EeaNR","colab_type":"code","colab":{}},"source":["##### RUN THE PIPELINE  #####  \n","\n","# DATA SELECTION FOR GRAKN TESTING\n","from mylib.data_analysis import ClassImbalance\n","\n","#data = UndersampleData(ALLDATA, max_sample = 100)\n","#data = UndersampleData(data, max_sample = 30) #at 30 you got 507 nx graphs created, howeve with NotDuct at this point\n","\n","# === 2 classes of 2000 sample 500/2500 ==== \n","data_sparse2 = ALLDATA[(ALLDATA.loc[:,'num_rays'] == 500) | (ALLDATA.loc[:,'num_rays'] == 2500)]\n","data = UndersampleData(data_sparse2, max_sample = 300)\n","data = data[:330]\n","\n","class_population = ClassImbalance(data, plot = True)\n","plt.show()\n","print(class_population)\n","\n","client = None\n","session = None\n","#KEYSPACE = 500n2500\n","\"\"\"\n","with session.transaction().read() as tx:\n","        # Change the terminology here onwards from thing -> node and role -> edge\n","        node_types = get_thing_types(tx)\n","        [node_types.remove(el) for el in TYPES_TO_IGNORE]\n","        edge_types = get_role_types(tx)\n","        [edge_types.remove(el) for el in ROLES_TO_IGNORE]\n","        print(f'Found node types: {node_types}')\n","        print(f'Found edge types: {edge_types}')\n","\"\"\"\n","node_types = ['SSP-vec', 'bottom-segment', 'duct', 'ray-input', 'source', 'sound-propagation-scenario', 'SSP_value', 'depth', 'location', 'season', 'grad', 'num_rays', 'length', 'slope', 'bottom_type', 'number_of_ducts', 'SSP-channel', 'convergence', 'src-position', 'bathymetry', 'sound-speed']\n","edge_types = ['has', 'channel_exists', 'define_SSP', 'find_channel', 'define_bathy', 'converged_scenario', 'defined_by_bathy', 'defined_by_src', 'minimum_resolution', 'define_src', 'defined_by_SSP']\n","\n","train_graphs, tr_ge_split, training_data, testing_data = prepare_data(session, data, \n","                                            train_split = 0.7, validation_split = 0., \n","                                            ubuntu_fix= True, savepath = SAVEPATH)\n","#, val_graphs,  val_ge_split\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzsqO1D5xswC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":702},"outputId":"3116334f-24a9-4949-bb88-fc62100ec2d1"},"source":["kgcn_vars = {\n","          'num_processing_steps_tr': 15,\n","          'num_processing_steps_ge': 15,\n","          'num_training_iterations': 5000, #10000?\n","          'learning_rate': 1e-4, # best at 1e-4\n","          'latent_size': 16, #MLP param 16\n","          'num_layers': 2, #MLP param 2 (try deeper configs)\n","          'clip': 5, #gradient clipping 5\n","          'weighted': False, #loss function modification\n","          'log_every_epochs': 50, #logging of the results\n","          'node_types': node_types,\n","          'edge_types': edge_types,\n","          'continuous_attributes': CONTINUOUS_ATTRIBUTES,\n","          'categorical_attributes': CATEGORICAL_ATTRIBUTES,\n","          'output_dir': f\"./events/500n2500/{time.time()}/\",\n","          'save_fle': \"training_summary.ckpt\" \n","          }           \n","\n","tf.reset_default_graph() #fix bugs with tensor of uknonw size\n","ge_graphs, solveds_tr, solveds_ge  = go_train(train_graphs, tr_ge_split, **kgcn_vars)\n","\n","#with session.transaction().write() as tx:\n","#        write_predictions_to_grakn(tr_ge_graphs, tx, commit = False)  # Write predictions to grakn with learned probabilities\n","    \n","#session.close()\n","#client.close()\n","\n","#val_ge_graphs, validation_evals = go_train(val_graphs, val_ge_split, reload_fle = \"test_model.ckpt\", **kgcn_vars)    \n","# Close transaction, session and client due to write query"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","TRAINING\n","\n","\n","Saving output to directory:events/500n2500/1598402395.446584/training_summary.ckpt\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QpqA9e3l3Fo2","colab_type":"code","colab":{}},"source":["#!git add KGCN/events\n","#!git commit -m \"colab training output\"\n","#!git remote add origin https://Qbbz:oczywelza13@github.com/Qbbz/SSP.git\n","#!git push -u origin colab-branch\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkBoQ8wgnRGs","colab_type":"code","colab":{}},"source":["!zip -r /content/events.zip /content/events/\n","from google.colab import files\n","files.download(\"/content/events.zip\")"],"execution_count":null,"outputs":[]}]}